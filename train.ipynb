{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BHZnXywjxGeq"
      },
      "outputs": [],
      "source": [
        "#https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kY_j8RUDP5y4"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import math\n",
        "from torch.autograd.variable import Variable\n",
        "import typing\n",
        "import random\n",
        "import os\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph2WO5JpKukk",
        "outputId": "4364d3af-8649-4bfa-c009-1e1b5b20918f"
      },
      "outputs": [],
      "source": [
        "prefix='/home/ericbuehler/english-transformer'\n",
        "prefix_='/home/ericbuehler/english-transformer'\n",
        "modelname=\"4_14_23_m3\"\n",
        "\n",
        "prefix_models=prefix+\"/models/\"+modelname+\"/\"\n",
        "\n",
        "if not os.path.exists(prefix_models):\n",
        "    os.makedirs(prefix_models)\n",
        "            \n",
        "os.chdir(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xGOoJdNdRSaO"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 24, 64])\n",
            "torch.Size([1, 24, 64])\n"
          ]
        }
      ],
      "source": [
        "x = torch.zeros(1,24,64)\n",
        "pos = PositionalEncoding(64)\n",
        "print(x.shape)\n",
        "print(pos(x).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XakHZ4HfFdYM"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        key_tp = key.transpose(-2, -1)\n",
        "\n",
        "        scores = query.matmul(key_tp) / math.sqrt(query.size()[-1])\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "            \n",
        "        attention = F.softmax(scores, dim = -1)\n",
        "\n",
        "        return attention.matmul(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G-sb6gd7L5bR"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 head_num,\n",
        "                 bias=True,\n",
        "                 activation=F.relu):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        if in_features % head_num != 0:\n",
        "            raise ValueError('`in_features`({}) should be divisible by \\\n",
        "                `head_num`({})'.format(in_features, head_num))\n",
        "        self.in_features = in_features\n",
        "        self.head_num = head_num\n",
        "        self.activation = activation\n",
        "        self.bias = bias\n",
        "        self.linear_q = nn.Linear(in_features, in_features, bias)\n",
        "        self.linear_k = nn.Linear(in_features, in_features, bias)\n",
        "        self.linear_v = nn.Linear(in_features, in_features, bias)\n",
        "        self.linear_o = nn.Linear(in_features, in_features, bias)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        q, k, v = self.linear_q(q), self.linear_k(k), self.linear_v(v)\n",
        "        if self.activation is not None:\n",
        "            q = self.activation(q)\n",
        "            k = self.activation(k)\n",
        "            v = self.activation(v)\n",
        "\n",
        "        q = self._reshape_to_batches(q)\n",
        "        k = self._reshape_to_batches(k)\n",
        "        v = self._reshape_to_batches(v)\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask.repeat(self.head_num, 1, 1)   \n",
        "        \n",
        "        y = ScaledDotProductAttention()(q, k, v, mask)        \n",
        "        \n",
        "        y = self._reshape_from_batches(y)      \n",
        "\n",
        "        y = self.linear_o(y)\n",
        "        if self.activation is not None:\n",
        "            y = self.activation(y)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def gen_causal_mask(x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        return torch.tril(torch.ones(seq_len, seq_len)).view(1, seq_len, seq_len).repeat(batch_size, 1, 1)\n",
        "\n",
        "    def _reshape_to_batches(self, x):\n",
        "        batch_size, seq_len, in_feature = x.size()\n",
        "        sub_dim = in_feature // self.head_num\n",
        "        return x.reshape(batch_size, seq_len, self.head_num, sub_dim)\\\n",
        "                .permute(0, 2, 1, 3)\\\n",
        "                .reshape(batch_size * self.head_num, seq_len, sub_dim)\n",
        "\n",
        "    def _reshape_from_batches(self, x):\n",
        "        batch_size, seq_len, in_feature = x.size()\n",
        "        batch_size //= self.head_num\n",
        "        out_dim = in_feature * self.head_num\n",
        "        return x.reshape(batch_size, self.head_num, seq_len, in_feature)\\\n",
        "                .permute(0, 2, 1, 3)\\\n",
        "                .reshape(batch_size, seq_len, out_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "NDpTASiaTxTX"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_dim: int,\n",
        "                 n_self_heads: int,\n",
        "                 n_cross_heads: int,\n",
        "                 n_features: int,\n",
        "                 n_layers: int):\n",
        "        super().__init__()\n",
        "\n",
        "        #Embedding layer\n",
        "        self.embedding = nn.Embedding(n_features, embedding_dim)\n",
        "        #Positional encoding\n",
        "        self.pos_encode = PositionalEncoding(embedding_dim)\n",
        "\n",
        "        self.encoder_layers = []\n",
        "        self.decoder_layers = []\n",
        "\n",
        "        for _ in range(n_layers):\n",
        "            layer = []\n",
        "            #Add multihead, which will be self attention\n",
        "            layer.append(MultiHeadAttention(embedding_dim, n_self_heads)) #self attention first, masked\n",
        "            #Now add layer norm\n",
        "            layer.append(nn.LayerNorm(embedding_dim))\n",
        "            #Add a feed forward\n",
        "            layer.append(nn.Linear(embedding_dim, embedding_dim))\n",
        "            #Now add layer norm\n",
        "            layer.append(nn.LayerNorm(embedding_dim))\n",
        "\n",
        "            self.encoder_layers.append(layer)\n",
        "\n",
        "        for _ in range(n_layers):\n",
        "            layer = []\n",
        "            #Add multihead, which will be cross or self attention\n",
        "            layer.append(MultiHeadAttention(embedding_dim, n_self_heads)) #self attention first, masked\n",
        "            #Now add layer norm\n",
        "            layer.append(nn.LayerNorm(embedding_dim))\n",
        "            layer.append(MultiHeadAttention(embedding_dim, n_cross_heads)) #cross attention second, not masked\n",
        "            #Now add layer norm\n",
        "            layer.append(nn.LayerNorm(embedding_dim))\n",
        "            #Add a feed forward\n",
        "            layer.append(nn.Linear(embedding_dim, embedding_dim))\n",
        "            #Now add layer norm\n",
        "            layer.append(nn.LayerNorm(embedding_dim))\n",
        "\n",
        "            self.decoder_layers.append(layer)\n",
        "\n",
        "        self.out = nn.Linear(embedding_dim, 1)\n",
        "            \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Expect tensor of [batch_size, n_features, embedding_dim]\n",
        "        \"\"\"\n",
        "        embed = self.embedding(x.long())\n",
        "        pos_encode = self.pos_encode(embed)\n",
        "\n",
        "        res = embed+pos_encode\n",
        "        base_res = res\n",
        "        \n",
        "        for (decoder_layer,encoder_layer) in zip(self.decoder_layers,self.encoder_layers):\n",
        "            e_self_attention = encoder_layer[0]\n",
        "            e_layer_norm_1 = encoder_layer[1]\n",
        "            e_ff = encoder_layer[2]\n",
        "            e_layer_norm_2 = encoder_layer[3]\n",
        "\n",
        "            d_self_attention = decoder_layer[0]\n",
        "            d_layer_norm_1 = decoder_layer[1]\n",
        "            d_cross_attention = decoder_layer[2]\n",
        "            d_layer_norm_2 = decoder_layer[3]\n",
        "            d_ff = decoder_layer[4]\n",
        "            d_layer_norm_3 = decoder_layer[5]\n",
        "\n",
        "            ## Run the encoder\n",
        "            #do self attention\n",
        "            encoder_res = res + e_self_attention(res,res,res)\n",
        "            e_self_res = encoder_res\n",
        "            #layer norm\n",
        "            encoder_res = e_layer_norm_1(encoder_res)\n",
        "            #do ff\n",
        "            encoder_res = e_self_res + e_ff(encoder_res)\n",
        "            #layer norm\n",
        "            encoder_res = e_layer_norm_2(encoder_res)\n",
        "\n",
        "            ######\n",
        "\n",
        "            ## Run the decoder\n",
        "            #do masked self attention\n",
        "            res = res + d_self_attention(res,res,res, mask = MultiHeadAttention.gen_causal_mask(res))\n",
        "            self_res = res\n",
        "            #layer norm\n",
        "            res = d_layer_norm_1(res)\n",
        "\n",
        "            #do cross attention \n",
        "            res = self_res + d_cross_attention(encoder_res,encoder_res,res)\n",
        "            cross_res = res\n",
        "            #layer norm\n",
        "            res = d_layer_norm_2(res)\n",
        "\n",
        "            #do ff\n",
        "            res = cross_res + d_ff(res)\n",
        "            #layer norm\n",
        "            res = d_layer_norm_3(res)\n",
        "\n",
        "        return self.out(res).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HqGGEpvCLbP8"
      },
      "outputs": [],
      "source": [
        "def tokenize_multi(text_seq: str, features: int, encoding = \"utf8\") -> torch.Tensor:\n",
        "    # tokenize the input text\n",
        "    sentences = []\n",
        "    for sentence in filter(lambda x: x!=\"\", text_seq.split(\"\\n\")):\n",
        "        base = list(bytes(sentence, \"utf8\"))\n",
        "        if len(base) < features:\n",
        "            base.extend([0] * (features - len(base)))\n",
        "        tensor = torch.Tensor(base)\n",
        "        tensor = tensor.unsqueeze(0)\n",
        "        sentences.append(tensor)\n",
        "\n",
        "    return torch.cat(sentences, dim = 0)\n",
        "\n",
        "def tokenize_single(sentence: str, features: int, encoding = \"utf8\") -> torch.Tensor:\n",
        "    base = list(bytes(sentence, \"utf8\"))\n",
        "    if len(base) < features:\n",
        "        base.extend([0] * (features - len(base)))\n",
        "    tensor = torch.Tensor(base)\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "R81mAag8mHEG"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data: typing.List[str], features):\n",
        "        self.raw_data = data\n",
        "        self.features = features\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        idx = -1\n",
        "        if self.raw_data[index].rfind(\" \") != -1:\n",
        "            idx = self.raw_data[index].rfind(\" \")\n",
        "\n",
        "        return (tokenize_single(self.raw_data[index][:idx], self.features), tokenize_single(self.raw_data[index], self.features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "97rbDfS4gO8C"
      },
      "outputs": [],
      "source": [
        "n_features = 262 # I know this is the largest\n",
        "embedding_dim = 384\n",
        "train_split = 0.9\n",
        "batch_size = 64\n",
        "head_factor = 64\n",
        "assert embedding_dim%head_factor == 0\n",
        "head_size = embedding_dim//head_factor\n",
        "n_layers = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "IvvenMeUKfaj"
      },
      "outputs": [],
      "source": [
        "path_to_data = \"data/english.txt\"\n",
        "data_raw = open(path_to_data, encoding=\"utf-8\").read()\n",
        "\n",
        "data_split = list(filter(lambda x: x!=\"\", data_raw.split(\"\\n\")))\n",
        "random.shuffle(data_split)\n",
        "\n",
        "n = int(train_split * len(data_split))\n",
        "train_data = data_split[:n]\n",
        "val_data = data_split[n:]\n",
        "\n",
        "train_dataloader = TextDataset(train_data, n_features)\n",
        "test_dataloader = TextDataset(train_data, n_features)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(train_dataloader, batch_size=batch_size)\n",
        "testloader = torch.utils.data.DataLoader(test_dataloader , batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvibftHanb0A",
        "outputId": "fcee2d44-26ea-4965-844b-4e1d9cb8b1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 262])\n",
            "torch.Size([1, 262])\n"
          ]
        }
      ],
      "source": [
        "model = Transformer(embedding_dim, head_size, head_size, n_features, n_layers)\n",
        "input = next(iter(testloader))[0]\n",
        "res = model(input)\n",
        "print(input.shape)\n",
        "print(res.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "lr = 1e-4  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/2179 [00:19<4:03:30,  6.71s/it]"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "epoch_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    batch_losses = []\n",
        "    for data, target in tqdm.tqdm(dataloader):\n",
        "        output = model(data)\n",
        "        \n",
        "        batch_losses.append(loss(output, target))\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "    epoch_losses.append(sum(batch_losses)/len(batch_losses))\n",
        "    print(f\"Epoch {epoch} completed with average loss: {epoch_losses[-1]}\")\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
